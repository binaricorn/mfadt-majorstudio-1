The ‘bias discussion’ always comes up when people talk about artificial intelligence (AI) and machine learning (ML). Its basic premise is that, just like any other technology, artificial intelligences are not ‘neutral’ but biased towards their creators. These biases are imposed upon the people who get exposed to the technology in some way or another.

For example, Google's computer vision service often recognizes handheld, lengthy, light-colored objects as burritos or Wii remotes. As artist Trevor Paglen notes in an essay, this is an indicator that the training dataset was created by youngish engineers in the San Francisco bay area, where the mission-style burrito was invented.

The bias discussions are sometimes hard to follow as they relate to concepts and issues from ethnography, feminism, demographics, etc.

While the basic concept of bias in neural networks is interesting, the implied complexities are deterring. My question is whether the topic can be explored on a basic level, leaving aside the larger issues. I imagine a project in the spirit of Taeyoon Choi's [Handmade Computer](http://taeyoonchoi.com/projects/handmade-computer/).

[MNIST](http://yann.lecun.com/exdb/mnist/) is a popular database of hand-written digits. It is frequently used as a exemplary training dataset in ML tutorials and software development.
Using MNIST, I want to create a simple project around biases in training sets or neural networks. According to the assignment, I will explore this ideas in various formal dimensions.

This approach could have multiple benefits: 
1. It could help create a basic understanding of algorithmic bias in a playful environment; for me and for others.
2. Not having to worry about offensive faux pas could be liberating to just make things.
2. It allows me to dive into ML workflows without the need for further resources (computing power, time, knowledge).